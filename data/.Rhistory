knitr::opts_chunk$set(include = FALSE)
# Check t-test assumptions
## Is noise normal?
hist(w.data$noise[w.data$location == 1])
# Set working directory here
setwd("C:/Users/bankh/My_Repos/habor-seal/data")
# Retrieve data
m.data<-read.csv("m.data.csv")
w.data<-read.csv("w.data.csv")
# Check t-test assumptions
## Is noise normal?
hist(w.data$noise[w.data$location == 1])
# Combine location avg noise and total seals within dates
# Set working directory here
setwd("C:/Users/bankh/My_Repos/habor-seal/data")
new.w.data<-read.csv("new.w.data.csv")
# Combine location avg noise and total seals within dates
# Set working directory here
setwd("C:/Users/bankh/My_Repos/habor-seal/data")
new.w.data<-read.csv("new.w.data.csv")
m.data<-read.csv("m.data.csv")
# Fix date
new.w.data$date<-as.Date(as.character(new.w.data$date),format = "%m/%d/%Y")
m.data$date<-as.Date(as.character(m.data$date),format = "%m/%d/%Y")
# Merge data
full.data<-merge(new.w.data,m.data,all = T)
summary(full.data)
full.data$time<-as.numeric(full.data$time)
full.data$month<-as.numeric(full.data$month)
## Store as csv
write.csv(full.data,"mw.data")
setwd("C:/Users/bankh/My_Repos/habor-seal/data")
full.data<-read.csv("mw.data.csv")
# Set working directory here
setwd("C:/Users/bankh/My_Repos/habor-seal/data")
# Retrieve data
m.data<-read.csv("m.data.csv")
w.data<-read.csv("w.data.csv")
# Combine location avg noise and total seals within dates
new.w.data<-read.csv("new.w.data.csv")
# Fix date
new.w.data$date<-as.Date(as.character(new.w.data$date),format = "%m/%d/%Y")
m.data$date<-as.Date(as.character(m.data$date),format = "%m/%d/%Y")
# Merge data
full.data<-merge(new.w.data,m.data,all = T)
full.data$time<-as.numeric(full.data$time)
full.data$month<-as.numeric(full.data$month)
## Create csv
write.csv(full.data,"full.data")
setwd("C:/Users/bankh/My_Repos/habor-seal/data")
full.data<-read.csv("full.data.csv")
# Run pairwise cor between all independent variables
## Cut-off is +/- 0.7
cor.matrix<-cor(full.data[,c(2:4,6:7)])
# Run pairwise cor between all independent variables
## Cut-off is +/- 0.7
cor.matrix<-cor(full.data[,c(2:4,6:7)])
cor.matrix
View(full.data)
# Combine location avg noise and total seals within dates
new.w.data<-read.csv("new.w.data.csv")
# Fix date
new.w.data$date<-as.Date(as.character(new.w.data$date),format = "%m/%d/%Y")
m.data$date<-as.Date(as.character(m.data$date),format = "%m/%d/%Y")
# Merge data
full.data<-merge(new.w.data,m.data,all = T)
summary(full.data)
full.data$time<-as.numeric(full.data$time)
# Set working directory here
setwd("C:/Users/bankh/My_Repos/habor-seal/data")
# Retrieve data
m.data<-read.csv("m.data.csv")
# Combine location avg noise and total seals within dates
new.w.data<-read.csv("new.w.data.csv")
# Fix date
new.w.data$date<-as.Date(as.character(new.w.data$date),format = "%m/%d/%Y")
m.data$date<-as.Date(as.character(m.data$date),format = "%m/%d/%Y")
# Merge data
full.data<-merge(new.w.data,m.data,all = T)
View(full.data)
# Run pairwise cor between all independent variables
## Cut-off is +/- 0.7
cor.matrix<-cor(full.data[,c(2:4,6:7)])
# Keep only large correlations in the same model
cor.matrix[abs(cor.matrix)< 0.7]<-NA
cor.matrix
setwd("C:/Users/bankh/My_Repos/habor-seal/data")
full.data<-read.csv("full.data.csv")
# Run pairwise cor between all independent variables
## Cut-off is +/- 0.7
cor.matrix<-cor(full.data[,c(2:4,6:7)])
setwd("C:/Users/bankh/My_Repos/habor-seal/data")
full.data<-read.csv("full.data.csv")
# Run pairwise cor between all independent variables
## Cut-off is +/- 0.7
cor.matrix<-cor(full.data[,c(2:4,6:7)])
# Keep only large correlations in the same model
cor.matrix[abs(cor.matrix)< 0.7]<-NA
cor.matrix
## No colilinearity
setwd("C:/Users/bankh/My_Repos/habor-seal/data")
full.data<-read.csv("full.data.csv")
# Check distribution
ggplot(full.data, aes(x=seals)) +
geom_histogram(aes(y=..density..), colour="black", fill="white")+
geom_density()+stat_density(alpha=.2,adjust = 1, fill="#FF6666")+xlab("Number of Seals Hauled-out")+ylab("Density")+theme(panel.background = element_blank())
setwd("C:/Users/bankh/My_Repos/habor-seal/data")
full.data<-read.csv("full.data.csv")
# Load packages
require(ggplot2)
# Check distribution
ggplot(full.data, aes(x=seals)) +
geom_histogram(aes(y=..density..), colour="black", fill="white")+
geom_density()+stat_density(alpha=.2,adjust = 1, fill="#FF6666")+xlab("Number of Seals Hauled-out")+ylab("Density")+theme(panel.background = element_blank())
## Negative binomial would be the best fit
setwd("C:/Users/bankh/My_Repos/habor-seal/data")
full.data<-read.csv("full.data.csv")
# Load packages
require(MASS) #for glm.nb()
model1<- glm.nb(seals ~ 1, data = full.data)
summary(model1)
model2<- glm.nb(seals ~ site*noise + month + tide + time, data = full.data)
summary(model2)
model3<- glm.nb(seals ~ site*noise + month + time, data = full.data)
summary(model3)
model4<- glm.nb(seals ~ site*noise + month, data = full.data)
summary(model4)
# Create list
models<-list(model1, model2, model3, model4)
## Calculate AICc with glm of models
AICc(models) # Looks like site*noise + month + time are the best predictors
## Find the AICc function
source("Functions.R")
## Find the AICc function
source("../code/Functions.R")
setwd("C:/Users/bankh/My_Repos/habor-seal/data")
full.data<-read.csv("full.data.csv")
# Load packages
require(MASS) #for glm.nb()
model1<- glm.nb(seals ~ 1, data = full.data)
summary(model1)
model2<- glm.nb(seals ~ site*noise + month + tide + time, data = full.data)
summary(model2)
model3<- glm.nb(seals ~ site*noise + month + time, data = full.data)
summary(model3)
model4<- glm.nb(seals ~ site*noise + month, data = full.data)
summary(model4)
# Create list
models<-list(model1, model2, model3, model4)
## Find the AICc function
source("../code/Functions.R")
## Calculate AICc with glm of models
AICc(models) # Looks like site*noise + month + time are the best predictors
###########################################################################
# AICc function
###########################################################################
AICc <- function(x){
## Calculate AICc with glm of models
n = length(x[[1]]$fitted)
AIC.table = c() # Make a place for summary table
for(i in 1:4) {
AIC.table<-
rbind(AIC.table, c(n, x[[i]]$rank + 1, x[[i]]$aic + (2*K*(K+1))/(n-K-1)))
}
colnames(AIC.table)<- c("N","df","AICc")
rownames(AIC.table)<- c("seals ~ 1",
"seals ~ site*noise + month + tide + time",
"seals ~ site*noise + month + time",
"seals ~ site*noise + month")
return(AIC.table)
}
# Check GLM
model1<- glm.nb(seals ~ 1, data = full.data)
summary(model1)
model2<- glm.nb(seals ~ site*noise + month + tide + time, data = full.data)
model3<- glm.nb(seals ~ site*noise + month + time, data = full.data)
model4<- glm.nb(seals ~ site*noise + month, data = full.data)
# Create list
models<-list(model1, model2, model3, model4)
## Find the AICc function
source("../code/Functions.R")
## Calculate AICc with glm of models
AICc(models) # Looks like site*noise + month + time are the best predictors
## Find the AICc function
source("../code/Functions.R")
## Calculate AICc with glm of models
AICc(models) # Looks like site*noise + month + time are the best predictors
setwd("C:/Users/bankh/My_Repos/habor-seal/data")
full.data<-read.csv("full.data.csv")
# Load packages
require(MASS) #for glm.nb()
model1<- glm.nb(seals ~ 1, data = full.data)
summary(model1)
model2<- glm.nb(seals ~ site*noise + month + tide + time, data = full.data)
summary(model2)
model3<- glm.nb(seals ~ site*noise + month + time, data = full.data)
summary(model3)
model4<- glm.nb(seals ~ site*noise + month, data = full.data)
summary(model4)
# Create list
models<-list(model1, model2, model3, model4)
## Find the AICc function
source("../code/Functions.R")
## Calculate AICc with glm of models
AICc(models) # Looks like site*noise + month + time are the best predictors
setwd("C:/Users/bankh/My_Repos/habor-seal/data")
full.data<-read.csv("full.data.csv")
# Load packages
require(MASS) #for glm.nb()
model3<- glm.nb(seals ~ site*noise + month + time, data = full.data)
summary(model3)
# Set working directory here
setwd("C:/Users/bankh/My_Repos/habor-seal/data")
new.w.data<-read.csv("new.w.data.csv")
# Load packages
require(ggplot2)
## Waterfront
ggplot(new.w.data, aes(noise,seals))+geom_point()+geom_smooth(method = "gam", formula =seals~s(noise))+
stat_smooth(method="gam",colour="black")+xlab("Average Noise Level (dB)")+ylab("Number of Seals Hauled-out")+
coord_trans(x = "log10")+theme(panel.background = element_blank())
# Set working directory here
setwd("C:/Users/bankh/My_Repos/habor-seal/data")
m.data<-read.csv("m.data.csv")
# Load packages
require(ggplot2)
## Marina
ggplot(m.data, aes(noise,seals))+geom_point()+geom_smooth(method = "gam", formula =seals~s(noise))+
stat_smooth(method="gam",colour="black")+xlab("Average Noise Level (dB)")+ylab("Number of Seals Hauled-out")+
coord_trans(x = "log10")+theme(panel.background = element_blank())
# Set working directory here
setwd("C:/Users/bankh/My_Repos/habor-seal/data")
new.w.data<-read.csv("new.w.data.csv")
# Load packages
require(ggplot2)
## Waterfront
p<-ggplot(new.w.data, aes(x=month,y =seals,group=month))
p+geom_boxplot(fill="black", alpha=0.2)+
xlab("Month")+ylab("Number of Seals")+theme(panel.background = element_blank())
# Set working directory here
setwd("C:/Users/bankh/My_Repos/habor-seal/data")
m.data<-read.csv("m.data.csv")
# Load packages
require(ggplot2)
## Marina
p<-ggplot(m.data, aes(x=month,y =seals,group=month))
p+geom_boxplot(fill="black", alpha=0.2)+
xlab("Month")+ylab("Number of Seals")+theme(panel.background = element_blank())
# Set working directory here
setwd("C:/Users/bankh/My_Repos/habor-seal/data")
# Retrieve data
m.data<-read.csv("m.data.csv")
w.data<-read.csv("w.data.csv")
# Check t-test assumptions
## Is noise normal?
hist(w.data$noise[w.data$location == 1])
hist(w.data$noise[w.data$location == 2])
### Both are normal
## Equal variance?
var(w.data$noise[w.data$location == 1])
var(w.data$noise[w.data$location == 2])
### Equal variance
# Set working directory here
setwd("C:/Users/bankh/My_Repos/habor-seal/data")
# Retrieve data
m.data<-read.csv("m.data.csv")
w.data<-read.csv("w.data.csv")
# Check t-test assumptions
## Is noise normal?
hist(w.data$noise[w.data$location == 1], breaks = 1)
hist(w.data$noise[w.data$location == 2], breaks = 1)
### Both are normal
## Equal variance?
var(w.data$noise[w.data$location == 1])
var(w.data$noise[w.data$location == 2])
### Equal variance
hist(w.data$noise[w.data$location == 1], breaks = 50)
hist(w.data$noise[w.data$location == 1], breaks = 10)
hist(w.data$noise[w.data$location == 1], breaks = 15)
# Set working directory here
setwd("C:/Users/bankh/My_Repos/habor-seal/data")
# Retrieve data
m.data<-read.csv("m.data.csv")
w.data<-read.csv("w.data.csv")
# Check t-test assumptions
## Is noise normal?
hist(w.data$noise[w.data$location == 1], breaks = 15)
hist(w.data$noise[w.data$location == 2], breaks = 15)
### Both are normal
## Equal variance?
var(w.data$noise[w.data$location == 1])
var(w.data$noise[w.data$location == 2])
### Equal variance
# Set working directory here
setwd("C:/Users/bankh/My_Repos/habor-seal/data")
# Retrieve data
m.data<-read.csv("m.data.csv")
w.data<-read.csv("w.data.csv")
# Check t-test assumptions
## Is noise normal?
hist(w.data$noise[w.data$location == 1], breaks = 15)
hist(w.data$noise[w.data$location == 2], breaks = 15)
### Both are normal
## Equal variance?
var(w.data$noise[w.data$location == 1])
var(w.data$noise[w.data$location == 2])
### Equal variance
unlink("C:/Users/bankh/My_Repos/habor-seal/results/GLM_Analysis_cache", recursive = TRUE)
knitr::opts_chunk$set(
message = TRUE,
warning = TRUE,
include = FALSE
)
# Set working directory here
setwd("C:/Users/bankh/My_Repos/habor-seal/data")
# Retrieve data
m.data<-read.csv("m.data.csv")
w.data<-read.csv("w.data.csv")
# Check t-test assumptions
## Is noise normal?
hist(w.data$noise[w.data$location == 1], breaks = 15)
hist(w.data$noise[w.data$location == 2], breaks = 15)
### Both are normal
## Equal variance?
var(w.data$noise[w.data$location == 1])
var(w.data$noise[w.data$location == 2])
### Equal variance
