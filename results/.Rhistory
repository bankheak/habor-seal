curve(dunif(x,min=2,max=44),from=0,to=75)
curve(dunif(x,min=2,max=44),from=0,to=60)
curve(punif(x,min=2,max=44),from=0,to=60)
curve(dnorm(x,mean=2,sd=4),from=0,to=60)
curve(dnorm(x,mean=2,sd=4),from=0,to=20)
curve(dnorm(x,mean=2,sd=4),from=0,to=10)
curve(dnorm(x,mean=2,sd=4),from=-10,to=20)
curve(pnorm(x,mean=2,sd=4),from=-10,to=20)
runif(x,min=2,max=44)
sd(runif(x,min=2,max=44))
var(rnorm(x,mean=2,sd=4))
mean(rnorm(x,mean=2,sd=4))
sd(dunif(x,min=2,max=44))
mean(dunif(x,min=2,max=44))
plot(dbinom(x,8,0.25))
plot(dbinom(x,8,0.25),from=0,to=8)
plot(pbinom(x,8,0.25))
plot(dpois(x,2))
plot(dpois(x,2),from=-5,to=8)
plot(dpois(x,2)),from=-5,to=8)
plot(ppois(x,2))
chisq.data<-c("red","blue","red","red","orange","blue","red","orange")
obs.freqs<-c(purple=154,orange=54)
exp.props<-c(purple=0.75,orange=0.25)
(exp.freqs<-sum(obs.freqs)*exp.props)
chisq.val<-sum(((obs.freqs-exp.freqs^2)/exp.freqs))
chisq.val
chisq.val<-sum(((obs.freqs-exp.freqs)^2)/exp.freqs)
chisq.val
curve(dchisq(x,df=1),0,20,ylab = "Density")
curve(dchisq(x, df = 1), 0, 20, ylab = "Density")
curve(dchisq(x, df = 1), ylab = "Density")
curve(dchisq(x, df = 1))
curve(dchisq(x, df = 1), 0, 20, ylab = "Density")
#Add chi-squared value to graph
abline(v = chisq.val, col = "firebrick4")
text(chisq.val, 0.1, "Test stat", pos = 4, col = "firebrick4")
#Add critical value to graph
abline(v = qchisq(0.95, df = 1), col = "steelblue")
text(qchisq(0.95, df = 1), 0.8, "Critical value", pos = 4, col = "steelblue")
plot.new()
frame()
frame()
curve(dchisq(x, df = 1), 0, 20, ylab = "Density")
#Add chi-squared value to graph
abline(v = chisq.val, col = "firebrick4")
text(chisq.val, 0.1, "Test stat", pos = 4, col = "firebrick4")
#Add critical value to graph
abline(v = qchisq(0.95, df = 1), col = "steelblue")
text(qchisq(0.95, df = 1), 0.8, "Critical value", pos = 4, col = "steelblue")
curve(dchisq(x, df = 1), 0, 20, ylab = "Density")
chisq.test(obs.freqs, p = exp.props, correct = F)
xVals<-barplot(obs.freqs,ylim = c(0,220),ylab="Frequency")
(chisq.data2 <- c(red = 4, blue = 2, orange = 2))
obs.freqs <- c(purple = 154, orange = 54)
exp.props <- c(purple = 0.75, orange = 0.25)
(exp.freqs <- sum(obs.freqs)*exp.props)
(chisq.val <- sum(((obs.freqs - exp.freqs)^2)/exp.freqs))
curve(dchisq(x, df = 1), 0, 20, ylab = "Density")
#Now change the degrees of freddom
curve(dchisq(x, df = 2), add = T, col = "steelblue")
curve(dchisq(x, df = 3), add = T, col = "steelblue1")
curve(dchisq(x, df = 1), 0, 20, ylab = "Density")
plot()
par(mar = rep(2, 4))
curve(dchisq(x, df = 1), 0, 20, ylab = "Density")
sponge<-c(Collencytes=58,Choanocytes=163,Porocytes=80)
sponge.obs<-c(Collencytes=58,Choanocytes=163,Porocytes=80)
sponge.exp<-c(Collencytes=0.22,Choanocytes=0.58,Porocytes=0.20)
(exp.freqs <- sum(sponge.obs)*sponge.exp)
(chisq.val <- sum(((sponge.obs - exp.freqs)^2)/exp.freqs))
1- pchisq(chisq.val, df = 2)
small<-c(Yes=23,No=67)
medium<-c(Yes=12,No=50)
large<-c(Yes=34,No=45)
(myData <- data.frame(small, medium,large))
(obsFreqsMar <- addmargins(myData))
obsFreqsMar <- addmargins(myData)
small <- c(yes = 23, no = 67)
medium <- c(yes = 12, no = 50)
large <- c(yes = 34, no = 45)
(obsFreqs <- matrix(c(small, medium,large), 2,
dimnames = list(hair = c("yes", "no"), size = c("s", "m","l"))))
(obsFreqsMar <- addmargins(obsFreqs))
(expFreqs <- outer(obsFreqsMar[1:2,3], obsFreqsMar[3,1:2])/obsFreqsMar[3,3])
(expFreqs <- outer(obsFreqsMar[1:3,4], obsFreqsMar[4,1:3])/obsFreqsMar[4,4])
(expFreqs <- outer(obsFreqsMar[1:3,4], obsFreqsMar[4,1:3])/obsFreqsMar[4,4])
(pChi <- 1 - pchisq(chiVal, 1))
(pChi <- 1 - pchisq(10.607, 2))
{23*ln(23/26.88)}+ {12*ln(12/18.52)}+ {34*ln(34/23.60)}+ {67*ln(67/63.12)}+ {50*ln(50/43.48)}+ {45*ln(45/55.40)}
{23*log(23/26.88)}+ {12*log(12/18.52)}+ {34*log(34/23.60)}+ {67*log(67/63.12)}+ {50*log(50/43.48)}+ {45*log(45/55.40)}
(pG <- 1 - pchisq(5.2478, 1))
{6*log(6/5.6)}+ {2*log(2/2.4)}+ {8*log(8/8.4)}+ {4*log(4/3.6)}
(pG <- 1 - pchisq(0.0804, 1))
{5.5*log(5.5/5.6)}+ {2.5*log(2.5/2.4)}+ {8.5*log(8.5/8.4)}+ {3.5*log(3.5/3.6)}
(pG <- 1 - pchisq(0.74, 1))
(pG <- 1 - pchisq(0.00495, 1))
curve(dchisq(x, 1), 0, 60, ylab = "Density")
curve(dchisq(x, 1), 0, 60, ylab = "Density")
abline(v = c(chiVal, gVal, qchisq(0.95, 1)), col = terrain.colors(3))
text(c(chiVal, gVal, qchisq(0.95, 1)), 0.3, c("Chi-squared value", "G value", "Critical value"), col = terrain.colors(3))
curve(dchisq(x, df = 2), 0, 20, ylab = "Density")
#Now change the degrees of freddom
curve(dchisq(x, df = 6), add = T, col = "blue")
curve(dchisq(x, df = 12), add = T, col = "red")
text(qchisq(0.95, df = 2), 0.8, "Critical value", pos = 4, col = "steelblue")
text(qchisq(0.95, df = 2), 0.8, add=T,"Critical value", pos = 4, col = "steelblue")
text(qchisq(0.95, df = 2), 0.8, "Critical value", pos = 4, col = "steelblue")
qchisq(0.95, df = 2)
qchisq(0.95, df = 2)
qchisq(0.95, df = 6)
qchisq(0.95, df = 12)
tData1<-read.csv(file.choose())
cc.species<-c(72.9, 40.9, 36.7, 64.2, 104.2, 33.6, 55.1, 44.3, 40.0, 91.1, 78.8)
meancc<-mean(cc.species)
secc<-sd(cc.species/sqrt(length(cc.species)))
secc<-sd(cc.species)/sqrt(length(cc.species))
(tcc<-(meancc-0)/secc)
t.test(cc.species,alternative="greater")
s1 <- c(23, 45, 34, 37, 29, 44, 40, 34)
s2 <- c(12, 20, 19, 18, 22, 14, 17, 17)
(twoSampleData <- data.frame(Sample1 = s1, Sample2 = s2))
sample <- rep(c("s1", "s2"), each = 8)
data <- c(23, 45, 34, 37, 29, 44, 40, 34, 12, 20, 19, 18, 22, 14, 17, 17)
(twoSampleData2 <- data.frame(Sample = sample, Data = data))
mean1 <- mean(twoSampleData$Sample1)
mean2 <- mean(twoSampleData$Sample2)
pooledVar <- (sum((twoSampleData$Sample1-mean1)^2) + sum((twoSampleData$Sample2-mean2)^2))/(length(twoSampleData$Sample1) + length(twoSampleData$Sample2) - 2)
pooledSE <- sqrt(pooledVar/length(twoSampleData$Sample1) + pooledVar/length(twoSampleData$Sample2))
(tTwoSample <- (mean1 - mean2)/pooledSE)
2*pt(-tTwoSample, length(twoSampleData$Sample1) + length(twoSampleData$Sample2) - 2)
se1 <- sd(twoSampleData$Sample1)/sqrt(length(twoSampleData$Sample1))
se2 <- sd(twoSampleData$Sample2)/sqrt(length(twoSampleData$Sample2))
barVals2 <- barplot(c(mean1, mean2),
ylim = c(0, 50),
ylab = "Label (units)",
names.arg = c("Sample 1", "Sample 2"))
arrows(barVals2, c(mean1+se1, mean2+se2), barVals2, c(mean1-se1, mean2-se2), angle = 90, code = 3)
dif <- twoSampleData$Sample1 - twoSampleData$Sample2
meanDif <- mean(dif)
seDif <- sd(dif)/sqrt(length(dif))
(tDif <- (meanDif - 0)/seDif)
chisq(11,4)
1-pchisq(11,4)
2*pt(1,20)
1-pt(11,1)
int2 <- 11
set.seed(int2)  #Sets the seed for the random numbers
data2 <- data.frame(
x2=gl(2, 12, labels = c("Exposed", "Protected")),
y2=round(c(rnorm(12, 20, 4), rnorm(12, 28, 4))))
data2           #These are your data
int2 <- 11
set.seed(int2)  #Sets the seed for the random numbers
data2 <- data.frame(
x2=gl(2, 12, labels = c("Exposed", "Protected")),
y2=round(c(rnorm(12, 20, 4), rnorm(12, 28, 4))))
data2           #These are your data
(twoSampleData2 <- data.frame(Sample = x2, Data = y2))
(twoSampleData2 <- data.frame(x2 = sample, y2 = data))
mean1 <- mean(data2$y2[data2$x2 == "Exposed"])
mean1
mean2 <- mean(data2$y2[data2$x2 == "Protected"])
mean2
pooledVar <- (sum((data2$y2[data2$x2 == "Exposed"]-mean1)^2) + sum((data2$y2[data2$x2 == "Protected"]-mean2)^2))/(length(data2$y2[data2$x2 == "Exposed"]) + length(data2$y2[data2$x2 == "Protected"]) - 2)
pooledVar
pooledSE <- sqrt(pooledVar/length(data2$y2[data2$x2 == "Exposed"]) + pooledVar/length(data2$y2[data2$x2 == "Protected"]))
pooledSE
(tTwoSample <- (mean1 - mean2)/pooledSE)
DF<-length(data2$y2[data2$x2 == "Exposed") + length(data2$y2[data2$x2 == "Protected") - 2
DF<-length(data2$y2[data2$x2 == "Exposed"]) + length(data2$y2[data2$x2 == "Protected") - 2
DF<-length(data2$x2 == "Exposed"]) + length(data2$y2[data2$x2 == "Protected") - 2
DF<-length(data2$y2[data2$x2 == "Exposed"]) + length(data2$y2[data2$x2 == "Protected"]) - 2
DF
t.test(data2$y2[data2$x2 == "Exposed"],data2$y2[data2$x2 == "Protected"],var.equal = T)
t.test(data2$y2[data2$x2 == "Exposed"],data2$y2[data2$x2 == "Protected"],var.equal = T,alternative = "less")
int3 <- 1
set.seed(int3)  #Sets the seed for the random numbers
data3 <- data.frame(samples=paste0("barnacle",1:8),
HighFlow= round(rnorm(8, int3+8, 4)),
LowFlow=round(rnorm(8, 13, 4)))
data3  #These are your data
dif <- data3$HighFlow - data3$LowFlow
dif
meanDif<-mean(dif)
meanDif
seDif <- sd(dif)/sqrt(length(dif))
seDif
sd(dif)
(tDif <- (meanDif - 0)/seDif)
df<-length(dif)-1
df
2*pt(tDif,df)
{[145-130.310]^2/130.310}+{[11-9.420]^2/9.420}+{[1- 17.270]^2/17.270}
pchisq(16.933,1)
1-pchisq(16.933,1)
1-pchisq(59.339,1)
int7 <- 1
set.seed(int7)  #Sets the seed for the random numbers
data7 <- data.frame(
x7=gl(2, 10, labels=c("Snail","No_Snail")),
y7=round(c(rnorm(10,42,18),rnorm(10,32,6)), 1))
data7           #These are your data
#install car package
install.packages("car")
require(car)
help("leveneTest")
data7
#How to use Levene’s test
leveneTest(y, group)
#where y = numerical data (data7$y7)
#	group = groups assignments (data7$x7)
leveneTest(data7$y7,data7$x7)
pleveneTest(4.818,18)
mean1 <- mean(data7$y7[data7$x7 == "Snail"])
mean2 <- mean(data7$y7[data7$x7 == "No_Snail"])
pooledVar <- (sum((data7$y7[data7$x7 == "Snail"]-mean1)^2) + sum((data7$y7[data7$x7 == "No_Snail"]-mean2)^2))/(length(data7$y7[data7$x7 == "Snail"]) + length(data7$y7[data7$x7 == "No_Snail"]) - 2)
pooledSE <- sqrt(pooledVar/length(data7$y7[data7$x7 == "Snail"]) + pooledVar/length(data7$y7[data7$x7 == "No_Snail"]))
(tTwoSample <- (mean1 - mean2)/pooledSE)
DF<-length(data7$y7[data7$x7 == "Snail"]) + length(data7$y7[data7$x7 == "No_Snail"]) - 2
DF
2*pt(2.229,18)
t.test(data7$y7[data7$x7 == "Snail"],data7$y7[data7$x7 == "No_Snail"],var.equal = F)
{11*log(11/9.52)}+ {6*log(6/7.48)}+ {3*log(3/4.48)}+ {5*log(5/3.52)}
1-pchisq(1.527,1)
1-pchisq(0.717,1)
install.packages("ggplot2")
require(ggplot2)
install.packages("ggplot2movies")
require(ggplot2movies)
data("movies")
str(movies)
qplot(x=budget,y=rating,data=movies)
qplot(year,budget,data=movies)
qplot(year,budget,data=movies,ylim=c(0,100000000),main="Movies are now expensive!",xlab = "Year movie was released",ylab = "Total budget (US dollars"))
qplot(year,budget,data=movies,ylim=c(0,100000000),main="Movies are now expensive!",xlab = "Year movie was released",ylab = "Total budget (US dollars)")
install.packages("ggthemes")
require(ggthemes)
data("iris")
ggplot(iris,aes(x=Species,y=Sepal.Length))+geom_boxplot()
plot(pnorm(x=c(0:20),10,1))
x<-c(0:20)
plot(pnorm(x,10,1))
x<-c(50:250)
plot(punif(x,100,210))
data<-c(456, 360, 612, 500, 587, 493,518)
mean(data)
median(data)
har.mean <- function(x){
length(x)/sum(1/x)
}
har.mean(data)
var(data)
quantile(data)
se <- function(x) sd(x)/sqrt(length(x))
se(data)
snow.data<-data.frame(Altitude=c(1234,1457,1637,1893,2011,2108,2383),Species.rich=c(4,5,9,5,7,18,4))
cor.test(snow.data$Altitude,snow.data$Species.rich)
cov(snow.data)
sd(snow.data$Altitude)
sd(snow.data$Species.rich)
crab <- iidspace(c("No Crab", "Crab"), ntrials = 80, probs = c(0.82, 0.18))
require(prob)
crab <- iidspace(c("No Crab", "Crab"), ntrials = 80, probs = c(0.82, 0.18))
require("prob")
crab <- iidspace(c("No Crab", "Crab"), ntrials = 80, probs = c(0.82, 0.18))
crab <- iidspace(c("No Crab", "Crab"), probs = c(0.82, 0.18))
crab <- iidspace(c("No Crab", "Crab"), ntrials = 10, probs = c(0.82, 0.18))
n<-80
x<-0:80
y<-pbinom(x,size=n,prob = 0.18)
plot(x,y)
pbinom(x,20,0.18)
dbinom(20,80,0.18)
pbinom(32,80,0.18)
1-pbinom(18,80,0.18)
pnorm(4,15,10)
1-pchisq(18,4)
ppois(18,8)-ppois(4,8)
ppois(4,8)-ppois(18,8)
1-punif(18,0,34.6)
pchisq(5.6088,1)
1-pchisq(5.6088,1)
ox.data<-data.frame(High=c(12.2,8.4,12,17.9,7.3),Med=c(26.4,21.7,6.0,16.2,18.0),Low=c(17.5,9.1,18.1,28.7,4.0))
View(ox.data)
summary(ox.data)
ox.data<-data.frame(Level=c("High","Med","Low"),Ox.con=c(High=c(12.2,8.4,12,17.9,7.3),Med=c(26.4,21.7,6.0,16.2,18.0),Low=c(17.5,9.1,18.1,28.7,4.0)))
fit_lm<-lm(Ox.con~Level,data=ox.data)
anova(fit_lm)
oxy.data<-read.csv(Book1)
oxy.data<-read.csv("Book1")
library(readr)
Book1 <- read_csv("Homework/Book1.csv")
View(Book1)
View(ox.data)
ox.data<-data.frame(Ox.con=c(High=c(12.2,8.4,12,17.9,7.3),Med=c(26.4,21.7,6.0,16.2,18.0),Low=c(17.5,9.1,18.1,28.7,4.0)))
ox.data<-data.frame(High=c(12.2,8.4,12,17.9,7.3),Med=c(26.4,21.7,6.0,16.2,18.0),Low=c(17.5,9.1,18.1,28.7,4.0))
str(ox.data)
ox.data<-data.frame(Level=c("High","Med","Low"),High=c(12.2,8.4,12,17.9,7.3),Med=c(26.4,21.7,6.0,16.2,18.0),Low=c(17.5,9.1,18.1,28.7,4.0))
1-pchisq(0.5143,1)
1-ppois(6,4)
dpois(2,4)
ppois(7,4)
s1<-c(40, 82, 45, 68, 42, 36, 64, 89, 71, 68)
s2<-c(23, 41, 34, 76, 32, 56, 63, 49, 51, 44)
(twoSampleData <- data.frame(Sample1 = s1, Sample2 = s2))
mean1 <- mean(twoSampleData$Sample1)
mean2 <- mean(twoSampleData$Sample2)
pooledVar <- (sum((twoSampleData$Sample1-mean1)^2) + sum((twoSampleData$Sample2-mean2)^2))/(length(twoSampleData$Sample1) + length(twoSampleData$Sample2) - 2)
pooledSE <- sqrt(pooledVar/length(twoSampleData$Sample1) + pooledVar/length(twoSampleData$Sample2))
(tTwoSample <- (mean1 - mean2)/pooledSE)
1-pt(1.768,18)
2*pt(12.3,4)
1-pt(2.3,18)
1-pf(2,18,4)
sample<-rep(c("High","Med","Low"),each=5)
ox.data<-c(12.2,8.4,12,17.9,7.3,26.4,21.7,6.0,16.2,18.0,17.5,9.1,18.1,28.7,4.0)
(threeSampleData<-data.frame(Sample=sample,Data=ox.data))
fit_lm<-lm(ox.data~sample,data=threeSampleData)
anova(fit_lm)
plot(ox.data~sample,data=threeSampleData)
boxplot(ox.data~sample,data=threeSampleData)
boxplot(ox.data~sample,data=threeSampleData)
alc.data<-data.frame(Predictor=c(10,20,30,40,50,60),Response=c(-1.7447,-1.824,-1.7696,-1.553,-1.377,-1.194))
fit<-lm(Response~Predictor,data=alc.data)
summary(fit)
fit$coefficients
source("~/OSU_Thesis/Sample Data/Coding_Organization.R")
source("~/OSU_Thesis/Sample Data/Coding_Organization.R")
source("~/OSU_Thesis/Sample Data/Coding_Organization.R")
source("~/OSU_Thesis/Sample Data/Coding_Organization.R")
source("~/OSU_Thesis/Sample Data/Coding_Organization.R")
sd_noise<-5
nobs<-100    # number of observations
slope<-0.1
intercept<--1
sd_noise<-5
predictor<-seq(0,100,length.out=nobs)
noise<-rnorm(nobs,mean=0,sd=sd_noise)
response<-intercept+slope*predictor+noise
plot(response,predictor)
plot(predictor,response)
source("~/OSU_Thesis/Sample Data/Coding_Organization.R")
dat<-data.frame(predictor=predictor,
response=response)
source("~/OSU_Thesis/Sample Data/Coding_Organization.R")
fit<-glm(response~predictor, data = dat)
source("~/OSU_Thesis/Sample Data/Coding_Organization.R")
plot(predictor, response)
abline(fit)
source("~/OSU_Thesis/Sample Data/Coding_Organization.R")
predictor<-runif(n = nobs,0,100)
source("~/OSU_Thesis/Sample Data/Coding_Organization.R")
predictor<-runif(n = nobs,0,100)
dat<-data.frame(predictor=predictor,
response=response)
pred <- predict(fit, newdata = dat)
source("~/OSU_Thesis/Sample Data/Coding_Organization.R")
# Remove all variables
rm(list=ls())
## load all necessary packages
install.packages("devtools")
install_github("whoppitt/NBDA")
require(devtools)
## load all necessary packages
require(rmarkdown)  # “Knit” button (Ctrl+Shift+K) displays preview
install.packages("microbenchmark")
install.packages("parallel")
install.packages("doParallel")
install.packages("foreach")
# Upload packages
require(ggplot2)
require(microbenchmark)
require(parallel)
require(doParallel)
require(foreach)
knitr::opts_chunk$set(echo = TRUE)
set.seed(1) # for reproducibility
geom_growth_base <- function(N0 = 2,
T = 999,
lambda = 1.01,
sigma = 0.2){
Nvals <- vector('numeric') # initiate a place to put the values
Nvals[1] <- N0
for (t in 1:T){
Nvals[t+1] <- Nvals[t]*(lambda*exp(rnorm(1,0,sigma)))
}
return(Nvals)
}
# Run the simulation
out <- geom_growth_base()
# Plot the results
plot(0:999,
out,
xlab='Time',
ylab='Population size',
type='o')
```{r}
# Default number of time-points
start_time <- Sys.time()
out <- geom_growth_base()
end_time <- Sys.time()
end_time - start_time
# Repeat with greater number of time-points
start_time <- Sys.time()
out <- geom_growth_base(T = 9E5)
out <- geom_growth_base(T = 9E5)
end_time <- Sys.time()
end_time <- Sys.time()
end_time - start_time
# Note that the time won't be exactly the same each time (unless the seed is the same)
start_time <- Sys.time()
out <- geom_growth_base(T = 9E5)
out <- geom_growth_base(T = 9E5)
end_time <- Sys.time()
end_time - start_time
system.time(geom_growth_base(T=9E5))
comp <- microbenchmark(TS_009 = {geom_growth_base(T = 9)},
TS_099 = {geom_growth_base(T = 99)},
TS_999 = {geom_growth_base(T = 999)})
comp
autoplot(comp)
set.seed(1) # for reproducibility
geom_growth_preallocated <- function(N0 = 2,
T = 999,
lambda = 1.01,
sigma = 0.2){
Nvals <- vector('numeric', length = T+1) # here's the only change
Nvals[1] <- N0
for (i in 1:T){
Nvals[i+1] <- Nvals[i]*(lambda*exp(rnorm(1,0,sigma)))
}
return(Nvals)
}
# Compare the old and new simulation functions
comp <- microbenchmark(Old = {geom_growth_base(T = 9999)},
New = {geom_growth_preallocated(T = 9999)})
comp
\subsection{Progress bar}
\subsection{Progress bar}
data <- geom_growth_preallocated(T = 99999)
start_time <- Sys.time()
growth_rates <- vector('numeric', (length(data)-1))
for(i in 1:(length(data)-1)){
growth_rates[i] <- data[i+1] / data[i]
}
end_time <- Sys.time()
end_time-start_time
start_time <- Sys.time()
growth_rates <- data[-1] / data[-length(data)]
end_time <- Sys.time()
end_time-start_time
n <- 5 # number of time-series to create
# use replicate() to create n time-series, each in a different matrix column
dat_array <- replicate(n, geom_growth_preallocated(T = 9999))
colnames(dat_array) <- paste0('Site_', 1:n)
head(dat_array)
calc_growth_rates <- function(x){
gr <- x[-1] / x[-length(x)]
return(gr)
}
system.time({
apply(dat_array, 2, calc_growth_rates)
})
dat_list <- as.list(as.data.frame(dat_array))
system.time({
growth_rates <- lapply(dat_list, calc_growth_rates)
})
lapply(growth_rates, head) # look only at head of each list element
growth_rate_means <- lapply( lapply(dat_list, calc_growth_rates), mean)
growth_rate_means
unlist(growth_rate_means)
growth_rate_means <- lapply( lapply(dat_list, calc_growth_rates), mean)
growth_rate_means
unlist(growth_rate_means)
sapply( lapply(dat_list, calc_growth_rates), mean)
The function \texttt{mapply} is useful when you want to parameterize a function from multiple vectors.
# Generate a large amount of demonstration data
n <- 1E8
data_list <- list("A" = rnorm(n),
"B" = rnorm(n),
"C" = rnorm(n),
"D" = rnorm(n))
data_list <- list("A" = rnorm(n),
"B" = rnorm(n),
"C" = rnorm(n),
"D" = rnorm(n))
detectCores()
cores <- 2
cl <- makeCluster(cores) # Create cluster
registerDoParallel(cl) # Activate clusters
system.time({
means <- foreach(i = 1:length(data_list),
.combine = c) %dopar% {
# replace c with rbind to create a dataframe
mean(data_list[[i]])
}
})
detectcores()
detect.cores()
summaryRprof()
detectCores()
m.data<-read.csv("../data/m.data.csv")
setwd("C:/Users/bankh/My_Repos/habor-seal/results")
m.data<-read.csv("../data/m.data.csv")
unlink("Diagnostics_cache", recursive = TRUE)
install.packages('tinytex')
require(tinytex)
update.packages(ask = FALSE, checkBuilt = TRUE)
tinytex::tlmgr_update()
tinytex::reinstall_tinytex()
install.packages("tlmgr")
